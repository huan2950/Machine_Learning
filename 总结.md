# BERT和GPT总结

## BERT

### 出道

- 优点：节省了从零开始训练语言处理模型所需要的时间、精力、知识和资源

- 开发步骤：
  - 下载预训练好的模型（unlabeled data）
  - 模型微调

### 应用

句子分类——只使用Encoder层，并且只关注第一个位置的输出，将输出的向量作为分类器的输入

垃圾邮件分类

语义分析——评价判断是正面还是负面

### 模型

BERT 基本上是一个**训练好的 Transformer 的 decoder 的栈**

BERT BASE:12层Encoder，前馈神经网络中有768个hidden layer（隐藏层），12个attention heads(注意力头)

BERT LARGE:20层Encoder，前馈神经网络中有1024个hidden layer，16个attention heads

### 词嵌入

==word2Vec==和==GloVe==:使用一个向量（一组数字）来恰当地表示单词，并捕捉单词的语义以及单词和单词之间的关系

==语境化的词嵌入==：不同于word2Vec和GloVe一个单词只表示为一个向量，根据不同的语境中单词不同的含义赋予不同的词嵌入

### 语言模型

==ELMo==：**双向语言模型**，使用在特定任务上经过训练的双向 LSTM 来创建这些词嵌入，通过训练，预测单词序列中的下一个词，从而获得了语言理解能力，这项任务被称为语言建模

==ULM-FiT==：提出了一个语言模型和一套流程，可以有效地为各种任务微调这个语言模型

==OpenAI Transformer==：**前向语言模型**，由 Transformer 的 Decoder 堆叠而成的，不会像普通的 Transformer 中的 Decoder 层那样有 Encoder-Decoder Attention 子层。不过，它仍然会有 Self Attention 层（这些层使用了 mask，因此不会看到句子后来的 token）

==Masked Language Model(MLM语言模型)==：使用Encoder，使用mask把需要预测的词屏蔽掉

## GPT

**GPT-2 是使用 Transformer 的 Decoder 模块构建的**。另一方面，**BERT 是使用 Transformer 的 Encoder 模块构建的**。

### GPT-2模型

1. 在嵌入矩阵中查找输入的单词的对应的 embedding 向量
2. 融入位置编码，知识单词在序列中的顺序
3. token先通过Masked Self Attention,再经过神经网络层，结果向量发送到下一个模块处理，每一个模块都是一个Decoder
4. 从模型顶部的模块产生的输出向量，和嵌入矩阵（**嵌入矩阵中的每一行都对应于模型词汇表中的一个词**）相乘，结果为模型词汇表中每个词的分数（概率）

### Self-Attention

- 为每个路径创建 Query、Key、Value 矩阵

，通过embedding向量和权重矩阵相乘创建 Query、Key 和 Value 向量

- 对于每个输入的 token，使用它的 Query 向量为所有其他的 Key 向量进行打分
- 将 Value 向量乘以它们对应的分数后求和

### Masked Self_attention

masking使用矩阵来实现，**将我们想要屏蔽的单元格设置为负无穷大或者一个非常大的负数**